{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pdb\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import losses\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Activation, Dense, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish Gaussians from Lorentzians, and predicting the mean, standard deviation if Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    #pdb.set_trace()\n",
    "    # Make 1000 new Gaussians to apply the model to\n",
    "    predX, predy = make_gaussians(1000)\n",
    "    \n",
    "    # Apply the model to get predicted means and sigmas of the Gaussians\n",
    "    pmeans, psigs = model.predict(predX, batch_size=None, verbose=0)\n",
    "    \n",
    "    # Check distribution of difference between true and predicted means, sigmas\n",
    "    plt.subplot(211)\n",
    "    _, _, _ = plt.hist(predy[:, 0] - pmeans.flatten(), bins=30)\n",
    "    plt.subplot(212)\n",
    "    _, _, _ = plt.hist(predy[:, 1] - psigs.flatten(), bins=30)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Check the relation between true and predicted means, sigmas\n",
    "    oto_means = np.linspace(-1., 1., 32) # one-to-one relation for means\n",
    "    oto_sigmas = np.linspace(0.25, 4.0, 32) # one-to-one relation for sigmas\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    # Plot the true y's and predicted y's from the NN model\n",
    "    plt.plot(predy[:, 0], pmeans.flatten(), marker='.')\n",
    "    # Plot the 1-to-1 line\n",
    "    plt.plot(oto_means, oto_means, color='black', ls='--')\n",
    "    plt.xlim(-1.0, 1.0)\n",
    "    plt.ylim(-1.0, 1.0)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(r'$\\mu$')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(predy[:, 1], psigs.flatten(), marker='.')\n",
    "    plt.plot(oto_sigmas, oto_sigmas, color='black', ls='--')\n",
    "    plt.xlim(0.25, 4.0)\n",
    "    plt.ylim(0.25, 4.0)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(r'$\\sigma$')\n",
    "    \n",
    "    #plt.savefig('gaussian_characteristics.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Gaussian given its mean and standard deviation\n",
    "def gaussian(x_vals, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x_vals - mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))\n",
    "\n",
    "# Make array that describes Gaussian\n",
    "def make_gaussians(num, mu_min=-1.0, mu_max=1.0, sig_min=0.25, sig_max=4.0): \n",
    "\n",
    "    means = np.random.uniform(mu_min, mu_max, num)\n",
    "    sigmas = np.random.uniform(sig_min, sig_max, num)\n",
    "\n",
    "    x_vals = np.linspace(-10.0, 10.0, 32)\n",
    "    models = np.zeros((num, 32))\n",
    "\n",
    "    for i in range(num):\n",
    "        models[i] = gaussian(x_vals, means[i], sigmas[i])\n",
    "    \n",
    "    # Also want to save and return the true means, sigmas used for the Gaussians\n",
    "    targets = np.vstack((means, sigmas)).T\n",
    "    \n",
    "    models = models.reshape(num, 32, 1)\n",
    "\n",
    "    return models, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Lorentzian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Lorentzian distribution given its location parameter and scale parameter\n",
    "def lorentzian(x_vals, loc, scale):\n",
    "    return ( 1/(np.pi*scale) ) * ( scale**2 / ((x_vals - loc)**2 + (scale)**2) )\n",
    "\n",
    "def make_lorentzians(num, loc_min=-1.0, loc_max=1.0, scale_min=0.25, scale_max=4.0):\n",
    "    \n",
    "    locs = np.random.uniform(loc_min, loc_max, num)\n",
    "    scales = np.random.uniform(scale_min, scale_max, num)\n",
    "    \n",
    "    x_vals = np.linspace(-10.0, 10.0, 32)\n",
    "    models = np.zeros((num, 32))\n",
    "    \n",
    "    for i in range(num):\n",
    "        models[i] = lorentzian(x_vals, locs[i], scales[i])\n",
    "        \n",
    "    targets = np.vstack((locs, scales)).T\n",
    "    \n",
    "    models = models.reshape(num, 32, 1)\n",
    "    \n",
    "    return models, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot to check Lorentzian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_vals = np.linspace(-10.0, 10.0, 2**8)\n",
    "\n",
    "plt.plot(x_vals, lorentzian(x_vals, 0.0, 1.0), label=r'x$_{0}=$0, $\\gamma=$1')\n",
    "plt.plot(x_vals, gaussian(x_vals, 0.0, 1.0), label=r'$\\mu=$0, $\\sigma=$1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0.0, 0.45)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "def generate_dataset(n_train=10000, n_test=1000, frac_gauss=None):\n",
    "    # From desired training and test set sizes, determine the fraction that is Gaussian vs Lorentzian\n",
    "    \n",
    "    # Random fraction w/limits to avoid too many of any one kind\n",
    "    if frac_gauss is None:\n",
    "        fg_train = np.random.uniform(0.25, 0.75)\n",
    "        fg_test = np.random.uniform(0.25, 0.75)\n",
    "    \n",
    "    # User-identified fraction\n",
    "    elif frac_gauss < 1.0:\n",
    "        fg_train = fg_test = frac_gauss\n",
    "        \n",
    "    num_gauss_train = int(fg_train * n_train)\n",
    "    num_gauss_test =  int(fg_test * n_test)\n",
    "    \n",
    "    num_lorentz_train = n_train - num_gauss_train\n",
    "    num_lorentz_test = n_test - num_gauss_test\n",
    "    \n",
    "    print (num_gauss_train, num_lorentz_train, num_gauss_test, num_lorentz_test)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    X_train_gauss, _ = make_gaussians(num_gauss_train)\n",
    "    X_test_gauss, _  = make_gaussians(num_gauss_test)\n",
    "\n",
    "    X_train_lorentz, _ = make_lorentzians(num_lorentz_train)\n",
    "    X_test_lorentz, _ = make_lorentzians(num_lorentz_test)\n",
    "    \n",
    "    # Classification\n",
    "    y_train = np.ones(n_train)\n",
    "    y_train[0:num_gauss_train] *= 0.0 # Gaussians are 0s\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    \n",
    "    y_test = np.ones(n_test)\n",
    "    y_test[0:num_gauss_test] *= 0.0 # Gaussians are 0s\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    # Combine Gaussian and Lorentzians\n",
    "    X_train = np.concatenate((X_train_gauss, X_train_lorentz))\n",
    "    X_test = np.concatenate((X_test_gauss, X_test_lorentz))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, compile, fit, and evaluate NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X_train, y_train, X_test, y_test = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "def baseline_model():\n",
    "    # Create model\n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(32, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    # There are 2 classes -- Gaussian or Lorentzian -- hence the Dense(2)\n",
    "#    model.add(Dense(2, kernel_initializer='normal', activation='softmax')) \n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(32, activation='relu')(inputs)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['val_accuracy'])\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, new_X, new_y = generate_dataset(n_train=10, n_test=1000)\n",
    "new_pred_y = model.predict(new_X)\n",
    "#new_pred_y = model.evaluate(new_X, new_y, verbose=1)\n",
    "\n",
    "print (new_pred_y)#, np.argmax(new_pred_y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify and Regress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Keras loss function that conditionally creates a zero gradient: https://stackoverflow.com/questions/54031644/custom-keras-loss-function-that-conditionally-creates-a-zero-gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "def generate_dataset_wparams(n_train=10000, n_test=1000, frac_gauss=None):\n",
    "    # From desired training and test set sizes, determine the fraction that is Gaussian vs Lorentzian\n",
    "    \n",
    "    # Random fraction w/limits to avoid too many of any one kind\n",
    "    if frac_gauss is None:\n",
    "        fg_train = np.random.uniform(0.25, 0.75)\n",
    "        fg_test = np.random.uniform(0.25, 0.75)\n",
    "    \n",
    "    # User-identified fraction\n",
    "    elif frac_gauss < 1.0:\n",
    "        fg_train = fg_test = frac_gauss\n",
    "        \n",
    "    num_gauss_train = int(fg_train * n_train)\n",
    "    num_gauss_test =  int(fg_test * n_test)\n",
    "    \n",
    "    num_lorentz_train = n_train - num_gauss_train\n",
    "    num_lorentz_test = n_test - num_gauss_test\n",
    "    \n",
    "    print (num_gauss_train, num_lorentz_train, num_gauss_test, num_lorentz_test)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    X_train_gauss, y_train_gauss = make_gaussians(num_gauss_train)\n",
    "    X_test_gauss, y_test_gauss = make_gaussians(num_gauss_test)\n",
    "\n",
    "    X_train_lorentz, y_train_lorentz = make_lorentzians(num_lorentz_train)\n",
    "    X_test_lorentz, y_test_lorentz = make_lorentzians(num_lorentz_test)\n",
    "\n",
    "    # Combine Gaussian and Lorentzians\n",
    "    X_train = np.concatenate((X_train_gauss, X_train_lorentz))\n",
    "    y_train = np.concatenate((y_train_gauss, y_train_lorentz))\n",
    "    X_test = np.concatenate((X_test_gauss, X_test_lorentz))\n",
    "    y_test = np.concatenate((y_test_gauss, y_test_lorentz))\n",
    "    \n",
    "    # Want to classify the arrays too...?\n",
    "    # Classification\n",
    "    class_train = np.ones(n_train)\n",
    "    class_train[0:num_gauss_train] *= 0.0 # Gaussians are 0s\n",
    "    #class_train = np_utils.to_categorical(class_train)\n",
    "    \n",
    "    class_test = np.ones(n_test)\n",
    "    class_test[0:num_gauss_test] *= 0.0 # Gaussians are 0s\n",
    "    #class_test = np_utils.to_categorical(class_test)    \n",
    "\n",
    "    return X_train, y_train, X_test, y_test, class_train, class_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4939 5061 442 558\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X_train, y_train, X_test, y_test, class_train, class_test = generate_dataset_wparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/54031644/custom-keras-loss-function-that-conditionally-creates-a-zero-gradient\n",
    "\n",
    "def custom_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "    # Want a loss that returns loss = 0.0 if the class is a Lorentzian (value of 1)\n",
    "        loss_val = losses.mean_squared_error(y_true, y_pred)\n",
    "        # K.switch(condition, then expression, else expression)\n",
    "        # If y_true is 0, then return a loss, else return 0.0\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        return K.switch(K.flatten(K.equal(y_true[:,0], 0.)), loss_val, K.zeros_like(loss_val))\n",
    "    \n",
    "    # Return a function\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "def baseline_model():\n",
    "    # Create model\n",
    "    verbose, epochs, batch_size = 1, 100, 16\n",
    "    input_shape = X_train.shape[1]\n",
    "\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    x = Conv1D(filters=16, kernel_size=3, activation='relu')(inputs)\n",
    "    x = Conv1D(filters=8, kernel_size=2, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)    \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    \n",
    "    classify = Dense(1, activation='sigmoid', name='type')(x)\n",
    "    regress1 = Dense(1, activation='linear', name='param1')(x)\n",
    "    regress2 = Dense(1, activation='linear', name='param2')(x)\n",
    "    \n",
    "    # Create model with input layer and dense layers\n",
    "    model = Model(inputs=inputs, outputs=[classify, regress1, regress2])\n",
    "    #model.compile(loss=[custom_loss(), custom_loss(), custom_loss()], optimizer='rmsprop')\n",
    "    model.compile(loss=['binary_crossentropy', 'mse', 'mse'], optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 1.2857 - type_loss: 0.6552 - param1_loss: 0.1247 - param2_loss: 0.5052\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.6249 - type_loss: 0.5132 - param1_loss: 0.0139 - param2_loss: 0.0974\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.3331 - type_loss: 0.2610 - param1_loss: 0.0087 - param2_loss: 0.0634\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.1639 - type_loss: 0.1134 - param1_loss: 0.0077 - param2_loss: 0.0428\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1068 - type_loss: 0.0627 - param1_loss: 0.0087 - param2_loss: 0.0353\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0827 - type_loss: 0.0434 - param1_loss: 0.0083 - param2_loss: 0.0309\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0651 - type_loss: 0.0320 - param1_loss: 0.0074 - param2_loss: 0.0256\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0538 - type_loss: 0.0250 - param1_loss: 0.0070 - param2_loss: 0.0220\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0448 - type_loss: 0.0204 - param1_loss: 0.0060 - param2_loss: 0.0184\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0378 - type_loss: 0.0171 - param1_loss: 0.0056 - param2_loss: 0.0151\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0333 - type_loss: 0.0153 - param1_loss: 0.0043 - param2_loss: 0.0137\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0292 - type_loss: 0.0131 - param1_loss: 0.0039 - param2_loss: 0.0121\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0254 - type_loss: 0.0101 - param1_loss: 0.0035 - param2_loss: 0.0118\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0235 - type_loss: 0.0093 - param1_loss: 0.0036 - param2_loss: 0.0107\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0213 - type_loss: 0.0084 - param1_loss: 0.0031 - param2_loss: 0.0099\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0191 - type_loss: 0.0062 - param1_loss: 0.0029 - param2_loss: 0.0100\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0169 - type_loss: 0.0045 - param1_loss: 0.0029 - param2_loss: 0.0095\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0155 - type_loss: 0.0036 - param1_loss: 0.0028 - param2_loss: 0.0090\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0150 - type_loss: 0.0034 - param1_loss: 0.0027 - param2_loss: 0.0089\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0132 - type_loss: 0.0027 - param1_loss: 0.0023 - param2_loss: 0.0082\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0123 - type_loss: 0.0018 - param1_loss: 0.0023 - param2_loss: 0.0083\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0116 - type_loss: 0.0016 - param1_loss: 0.0020 - param2_loss: 0.0080\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0111 - type_loss: 0.0010 - param1_loss: 0.0020 - param2_loss: 0.0081\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0103 - type_loss: 7.7853e-04 - param1_loss: 0.0018 - param2_loss: 0.0077\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0097 - type_loss: 8.9620e-04 - param1_loss: 0.0018 - param2_loss: 0.0070\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0093 - type_loss: 4.3204e-04 - param1_loss: 0.0016 - param2_loss: 0.0073\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0088 - type_loss: 2.7211e-04 - param1_loss: 0.0015 - param2_loss: 0.0070\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0086 - type_loss: 3.8816e-04 - param1_loss: 0.0014 - param2_loss: 0.0068\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 1s 106us/step - loss: 0.0084 - type_loss: 9.6084e-04 - param1_loss: 0.0013 - param2_loss: 0.0062ETA: 1s - loss: 0.0109 - type_loss: 2.5760e-04 - param1_loss: 0.0019 - par\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0080 - type_loss: 6.1782e-04 - param1_loss: 0.0013 - param2_loss: 0.0061\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 1s 104us/step - loss: 0.0077 - type_loss: 1.9945e-04 - param1_loss: 0.0013 - param2_loss: 0.0063\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0073 - type_loss: 2.7370e-04 - param1_loss: 0.0011 - param2_loss: 0.0059\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0069 - type_loss: 1.0832e-04 - param1_loss: 0.0011 - param2_loss: 0.0057\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0070 - type_loss: 2.9617e-04 - param1_loss: 9.6840e-04 - param2_loss: 0.0057\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0066 - type_loss: 2.0630e-04 - param1_loss: 0.0010 - param2_loss: 0.0053\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0066 - type_loss: 2.3297e-04 - param1_loss: 9.3973e-04 - param2_loss: 0.0054\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0060 - type_loss: 1.2656e-04 - param1_loss: 9.4976e-04 - param2_loss: 0.0049\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0061 - type_loss: 6.4108e-05 - param1_loss: 8.8799e-04 - param2_loss: 0.0051\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0058 - type_loss: 4.6848e-05 - param1_loss: 9.6570e-04 - param2_loss: 0.0048\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0057 - type_loss: 4.5242e-05 - param1_loss: 9.1732e-04 - param2_loss: 0.0047\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0056 - type_loss: 2.8471e-04 - param1_loss: 8.5121e-04 - param2_loss: 0.0045\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0054 - type_loss: 1.9151e-04 - param1_loss: 8.3006e-04 - param2_loss: 0.0044\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0055 - type_loss: 4.0754e-05 - param1_loss: 8.2557e-04 - param2_loss: 0.0046\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0053 - type_loss: 6.4315e-05 - param1_loss: 7.9917e-04 - param2_loss: 0.0044\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0051 - type_loss: 1.1223e-04 - param1_loss: 7.4801e-04 - param2_loss: 0.0042\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0050 - type_loss: 2.6727e-05 - param1_loss: 7.6619e-04 - param2_loss: 0.0042\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0049 - type_loss: 3.7105e-05 - param1_loss: 7.0334e-04 - param2_loss: 0.0041\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0047 - type_loss: 3.1763e-05 - param1_loss: 7.6227e-04 - param2_loss: 0.0039\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0046 - type_loss: 1.1275e-04 - param1_loss: 7.0735e-04 - param2_loss: 0.0037\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0045 - type_loss: 2.8500e-04 - param1_loss: 6.8155e-04 - param2_loss: 0.0036\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0046 - type_loss: 3.4984e-04 - param1_loss: 6.1175e-04 - param2_loss: 0.0036\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0044 - type_loss: 4.8196e-05 - param1_loss: 6.0989e-04 - param2_loss: 0.0037\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0042 - type_loss: 2.3596e-05 - param1_loss: 6.3848e-04 - param2_loss: 0.0036\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0041 - type_loss: 2.5124e-05 - param1_loss: 6.3742e-04 - param2_loss: 0.0034\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0041 - type_loss: 1.8425e-04 - param1_loss: 6.0086e-04 - param2_loss: 0.0033\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0040 - type_loss: 2.0613e-04 - param1_loss: 5.8632e-04 - param2_loss: 0.0032\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0038 - type_loss: 1.8391e-05 - param1_loss: 5.7461e-04 - param2_loss: 0.0032\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0039 - type_loss: 4.0111e-05 - param1_loss: 5.5922e-04 - param2_loss: 0.0033\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0037 - type_loss: 2.9078e-05 - param1_loss: 5.6240e-04 - param2_loss: 0.0031\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0036 - type_loss: 2.7111e-05 - param1_loss: 5.3667e-04 - param2_loss: 0.0031\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0035 - type_loss: 1.7451e-05 - param1_loss: 5.3946e-04 - param2_loss: 0.0030\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0035 - type_loss: 1.7152e-05 - param1_loss: 5.2719e-04 - param2_loss: 0.0030\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0034 - type_loss: 1.2371e-04 - param1_loss: 5.3254e-04 - param2_loss: 0.0028\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0034 - type_loss: 2.3278e-05 - param1_loss: 5.3223e-04 - param2_loss: 0.0028\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0033 - type_loss: 8.0866e-05 - param1_loss: 5.0471e-04 - param2_loss: 0.0027\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0033 - type_loss: 3.1240e-05 - param1_loss: 5.0577e-04 - param2_loss: 0.0028\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0032 - type_loss: 3.5030e-05 - param1_loss: 4.8094e-04 - param2_loss: 0.0027\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0032 - type_loss: 1.1507e-05 - param1_loss: 4.8539e-04 - param2_loss: 0.0027\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0031 - type_loss: 1.0971e-05 - param1_loss: 4.8742e-04 - param2_loss: 0.0026\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0030 - type_loss: 2.7817e-05 - param1_loss: 4.7443e-04 - param2_loss: 0.0025\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0030 - type_loss: 1.4327e-05 - param1_loss: 4.7196e-04 - param2_loss: 0.0025\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0030 - type_loss: 1.3592e-05 - param1_loss: 4.6224e-04 - param2_loss: 0.0026\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0029 - type_loss: 1.1641e-05 - param1_loss: 4.4006e-04 - param2_loss: 0.0024\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0029 - type_loss: 9.2037e-06 - param1_loss: 4.5124e-04 - param2_loss: 0.0024\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0029 - type_loss: 7.5698e-06 - param1_loss: 4.2933e-04 - param2_loss: 0.0024\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0028 - type_loss: 1.1845e-05 - param1_loss: 4.4153e-04 - param2_loss: 0.0024\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0028 - type_loss: 1.4016e-05 - param1_loss: 4.3845e-04 - param2_loss: 0.0024\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0028 - type_loss: 8.3165e-06 - param1_loss: 4.1030e-04 - param2_loss: 0.0024\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0027 - type_loss: 7.4522e-06 - param1_loss: 4.2772e-04 - param2_loss: 0.0023\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0026 - type_loss: 5.1806e-05 - param1_loss: 4.2527e-04 - param2_loss: 0.0021\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0027 - type_loss: 6.4440e-06 - param1_loss: 4.1248e-04 - param2_loss: 0.0022\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0026 - type_loss: 7.1557e-06 - param1_loss: 4.1097e-04 - param2_loss: 0.0022\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0026 - type_loss: 6.0892e-06 - param1_loss: 4.1245e-04 - param2_loss: 0.0022\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0026 - type_loss: 5.3906e-06 - param1_loss: 4.1140e-04 - param2_loss: 0.0022\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0025 - type_loss: 4.4289e-06 - param1_loss: 4.0154e-04 - param2_loss: 0.0021\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0025 - type_loss: 9.1872e-06 - param1_loss: 3.8932e-04 - param2_loss: 0.0021\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0025 - type_loss: 1.0471e-05 - param1_loss: 3.7089e-04 - param2_loss: 0.0021\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0024 - type_loss: 3.8966e-06 - param1_loss: 3.8609e-04 - param2_loss: 0.0020\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0025 - type_loss: 7.4315e-06 - param1_loss: 3.8458e-04 - param2_loss: 0.0021\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0023 - type_loss: 5.2006e-06 - param1_loss: 3.8899e-04 - param2_loss: 0.0019\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0024 - type_loss: 1.0545e-05 - param1_loss: 3.7705e-04 - param2_loss: 0.0020\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0023 - type_loss: 6.0204e-06 - param1_loss: 3.8088e-04 - param2_loss: 0.0019\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0023 - type_loss: 3.8329e-06 - param1_loss: 3.6717e-04 - param2_loss: 0.0019\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0024 - type_loss: 3.1562e-06 - param1_loss: 3.6317e-04 - param2_loss: 0.0020\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0023 - type_loss: 1.0410e-05 - param1_loss: 3.5982e-04 - param2_loss: 0.0019\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0023 - type_loss: 8.0339e-06 - param1_loss: 3.4967e-04 - param2_loss: 0.0019\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0023 - type_loss: 2.9871e-06 - param1_loss: 3.5944e-04 - param2_loss: 0.0019\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0022 - type_loss: 2.7240e-06 - param1_loss: 3.5256e-04 - param2_loss: 0.0018\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0022 - type_loss: 3.4140e-06 - param1_loss: 3.4613e-04 - param2_loss: 0.0019\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0021 - type_loss: 1.2916e-05 - param1_loss: 3.3341e-04 - param2_loss: 0.0018\n",
      "Baseline Error: 100.00%\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 1, 100, 16\n",
    "\n",
    "# Build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, [np.vstack(class_train), y_train[:,0], y_train[:,1]], epochs=epochs, verbose=verbose)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, [np.vstack(class_test), y_test[:,0], y_test[:,1]], verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 7 69 31\n"
     ]
    }
   ],
   "source": [
    "_, _, new_X, new_y, new_ctr, new_cte = generate_dataset_wparams(n_train=10, n_test=100)\n",
    "new_pred_y = model.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ctr, new_cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(new_pred_y[0].flatten()- new_cte, 'o')\n",
    "plt.ylim(-0.1, 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following classification+regression (gender+age) problem from online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify and regress at the same time: https://stats.stackexchange.com/questions/77330/classify-and-regress-at-the-same-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')\n",
    "adult.columns = ['age', 'workclass', 'fnlwgt', 'edu', 'edu_num', 'marital_status',\n",
    "                 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "                 'hours_per_week', 'native_country', 'income']\n",
    "adult[\"sex\"] = adult[\"sex\"].astype('category').cat.codes\n",
    "adult[\"workclass\"] = adult[\"workclass\"].astype('category').cat.codes\n",
    "adult[\"marital_status\"] = adult[\"marital_status\"].astype('category').cat.codes\n",
    "adult[\"race\"] = adult[\"race\"].astype('category').cat.codes\n",
    "adult[\"occupation\"] = adult[\"occupation\"].astype('category').cat.codes\n",
    "adult[\"native_country\"] = adult[\"native_country\"].astype('category').cat.codes\n",
    "\n",
    "target_bin = adult[['sex']].values.astype('float32') # 0 is female?\n",
    "target_num = adult[['age']].values.astype('float32')\n",
    "X = adult[['workclass', 'edu_num', 'marital_status', 'occupation', 'race', 'capital_gain',\n",
    "           'capital_loss', 'hours_per_week', 'native_country']].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "inputs = Input(shape=(X.shape[1],), name='Input')\n",
    "\n",
    "hidden = Dense(64, name='Shared-Hidden-Layer', activation='relu')(inputs)\n",
    "hidden = BatchNormalization()(hidden)\n",
    "out_bin = Dense(1, name='Output-Bin', activation='sigmoid')(hidden)\n",
    "out_num = Dense(1, name='Output-Num', activation='linear')(hidden)\n",
    "\n",
    "model = Model(inputs, [out_bin, out_num])\n",
    "model.compile(optimizer=Adam(0.10), loss=['binary_crossentropy', 'mean_squared_error'])\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, [target_bin, target_num], validation_split=.20, epochs=100, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gend = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Fraction incorrectly gendered:', 100*len(np.where(np.around(gend[0].flatten()) - target_bin.flatten() != 0.)[0]) / len(target_bin.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import losses\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    loss = losses.mean_squared_error(y_true, y_pred)\n",
    "    pdb.set_trace()\n",
    "    return K.switch(K.flatten(K.equal(y_true, 0.)), K.zeros_like(loss), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1, input_shape=(1,)))\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "weights, bias = model.layers[0].get_weights()\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([0, 0, 0])\n",
    "\n",
    "model.train_on_batch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.keras.losses.mean_squared_error(tf.ones((2, 2,)), tf.zeros((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.equal(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
