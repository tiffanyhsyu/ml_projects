{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pdb\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish Gaussians from Lorentzians, and predicting the mean, standard deviation if Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    #pdb.set_trace()\n",
    "    # Make 1000 new Gaussians to apply the model to\n",
    "    predX, predy = make_gaussians(1000)\n",
    "    \n",
    "    # Apply the model to get predicted means and sigmas of the Gaussians\n",
    "    pmeans, psigs = model.predict(predX, batch_size=None, verbose=0)\n",
    "    \n",
    "    # Check distribution of difference between true and predicted means, sigmas\n",
    "    plt.subplot(211)\n",
    "    _, _, _ = plt.hist(predy[:, 0] - pmeans.flatten(), bins=30)\n",
    "    plt.subplot(212)\n",
    "    _, _, _ = plt.hist(predy[:, 1] - psigs.flatten(), bins=30)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Check the relation between true and predicted means, sigmas\n",
    "    oto_means = np.linspace(-1., 1., 32) # one-to-one relation for means\n",
    "    oto_sigmas = np.linspace(0.25, 4.0, 32) # one-to-one relation for sigmas\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    # Plot the true y's and predicted y's from the NN model\n",
    "    plt.plot(predy[:, 0], pmeans.flatten(), marker='.')\n",
    "    # Plot the 1-to-1 line\n",
    "    plt.plot(oto_means, oto_means, color='black', ls='--')\n",
    "    plt.xlim(-1.0, 1.0)\n",
    "    plt.ylim(-1.0, 1.0)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(r'$\\mu$')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(predy[:, 1], psigs.flatten(), marker='.')\n",
    "    plt.plot(oto_sigmas, oto_sigmas, color='black', ls='--')\n",
    "    plt.xlim(0.25, 4.0)\n",
    "    plt.ylim(0.25, 4.0)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(r'$\\sigma$')\n",
    "    \n",
    "    #plt.savefig('gaussian_characteristics.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Gaussian given its mean and standard deviation\n",
    "def gaussian(x_vals, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x_vals - mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))\n",
    "\n",
    "# Make array that describes Gaussian\n",
    "def make_gaussians(num, mu_min=-1.0, mu_max=1.0, sig_min=0.25, sig_max=4.0): \n",
    "\n",
    "    means = np.random.uniform(mu_min, mu_max, num)\n",
    "    sigmas = np.random.uniform(sig_min, sig_max, num)\n",
    "\n",
    "    x_vals = np.linspace(-10.0, 10.0, 32)\n",
    "    models = np.zeros((num, 32))\n",
    "\n",
    "    for i in range(num):\n",
    "        models[i] = gaussian(x_vals, means[i], sigmas[i])\n",
    "    \n",
    "    # Also want to save and return the true means, sigmas used for the Gaussians\n",
    "    targets = np.vstack((means, sigmas)).T\n",
    "\n",
    "    return models, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Lorentzian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Lorentzian distribution given its location parameter and scale parameter\n",
    "def lorentzian(x_vals, loc, scale):\n",
    "    return ( 1/(np.pi*scale) ) * ( scale**2 / ((x_vals - loc)**2 + (scale)**2) )\n",
    "\n",
    "def make_lorentzians(num, loc_min=-1.0, loc_max=1.0, scale_min=0.25, scale_max=4.0):\n",
    "    \n",
    "    locs = np.random.uniform(loc_min, loc_max, num)\n",
    "    scales = np.random.uniform(scale_min, scale_max, num)\n",
    "    \n",
    "    x_vals = np.linspace(-10.0, 10.0, 32)\n",
    "    models = np.zeros((num, 32))\n",
    "    \n",
    "    for i in range(num):\n",
    "        models[i] = lorentzian(x_vals, locs[i], scales[i])\n",
    "        \n",
    "    targets = np.vstack((locs, scales)).T\n",
    "    \n",
    "    return models, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot to check Lorentzian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_vals = np.linspace(-10.0, 10.0, 2**8)\n",
    "\n",
    "plt.plot(x_vals, lorentzian(x_vals, 0.0, 1.0), label=r'x$_{0}=$0, $\\gamma=$1')\n",
    "plt.plot(x_vals, gaussian(x_vals, 0.0, 1.0), label=r'$\\mu=$0, $\\sigma=$1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0.0, 0.45)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "def generate_dataset(n_train=10000, n_test=1000, frac_gauss=None):\n",
    "    # From desired training and test set sizes, determine the fraction that is Gaussian vs Lorentzian\n",
    "    \n",
    "    # Random fraction w/limits to avoid too many of any one kind\n",
    "    if frac_gauss is None:\n",
    "        fg_train = np.random.uniform(0.25, 0.75)\n",
    "        fg_test = np.random.uniform(0.25, 0.75)\n",
    "    \n",
    "    # User-identified fraction\n",
    "    elif frac_gauss < 1.0:\n",
    "        fg_train = fg_test = frac_gauss\n",
    "        \n",
    "    num_gauss_train = int(fg_train * n_train)\n",
    "    num_gauss_test =  int(fg_test * n_test)\n",
    "    \n",
    "    num_lorentz_train = n_train - num_gauss_train\n",
    "    num_lorentz_test = n_test - num_gauss_test\n",
    "    \n",
    "    print (num_gauss_train, num_lorentz_train, num_gauss_test, num_lorentz_test)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    X_train_gauss, _ = make_gaussians(num_gauss_train)\n",
    "    X_test_gauss, _  = make_gaussians(num_gauss_test)\n",
    "\n",
    "    X_train_lorentz, _ = make_lorentzians(num_lorentz_train)\n",
    "    X_test_lorentz, _ = make_lorentzians(num_lorentz_test)\n",
    "    \n",
    "    # Classification\n",
    "    y_train = np.ones(n_train)\n",
    "    y_train[0:num_gauss_train] *= 0.0 # Gaussians are 0s\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    \n",
    "    y_test = np.ones(n_test)\n",
    "    y_test[0:num_gauss_test] *= 0.0 # Gaussians are 0s\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    # Combine Gaussian and Lorentzians\n",
    "    X_train = np.concatenate((X_train_gauss, X_train_lorentz))\n",
    "    X_test = np.concatenate((X_test_gauss, X_test_lorentz))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, compile, fit, and evaluate NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6292 3708 632 368\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X_train, y_train, X_test, y_test = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "def baseline_model():\n",
    "    # Create model\n",
    "#    model = Sequential()\n",
    "#    model.add(Dense(32, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    # There are 2 classes -- Gaussian or Lorentzian -- hence the Dense(2)\n",
    "#    model.add(Dense(2, kernel_initializer='normal', activation='softmax')) \n",
    "    \n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(32, activation='relu')(inputs)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.6566 - accuracy: 0.6137 - val_loss: 0.6282 - val_accuracy: 0.6320\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.6149 - accuracy: 0.6292 - val_loss: 0.5895 - val_accuracy: 0.6320\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.5697 - accuracy: 0.6752 - val_loss: 0.5367 - val_accuracy: 0.7430\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.5110 - accuracy: 0.7750 - val_loss: 0.4710 - val_accuracy: 0.7970\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.4399 - accuracy: 0.8197 - val_loss: 0.3990 - val_accuracy: 0.8510\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.3674 - accuracy: 0.8773 - val_loss: 0.3349 - val_accuracy: 0.9270\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.3018 - accuracy: 0.9291 - val_loss: 0.2730 - val_accuracy: 0.9470\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.2475 - accuracy: 0.9581 - val_loss: 0.2251 - val_accuracy: 0.9770\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.2030 - accuracy: 0.9762 - val_loss: 0.1846 - val_accuracy: 0.9800\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.1669 - accuracy: 0.9809 - val_loss: 0.1533 - val_accuracy: 0.9890\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.1381 - accuracy: 0.9848 - val_loss: 0.1275 - val_accuracy: 0.9830\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.1154 - accuracy: 0.9886 - val_loss: 0.1068 - val_accuracy: 0.9880\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0977 - accuracy: 0.9905 - val_loss: 0.0912 - val_accuracy: 0.9920\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0836 - accuracy: 0.9926 - val_loss: 0.0790 - val_accuracy: 0.9940\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0725 - accuracy: 0.9930 - val_loss: 0.0690 - val_accuracy: 0.9950\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0632 - accuracy: 0.9936 - val_loss: 0.0605 - val_accuracy: 0.9950\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0557 - accuracy: 0.9940 - val_loss: 0.0540 - val_accuracy: 0.9950\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0498 - accuracy: 0.9945 - val_loss: 0.0479 - val_accuracy: 0.9950\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0444 - accuracy: 0.9945 - val_loss: 0.0428 - val_accuracy: 0.9960\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0400 - accuracy: 0.9952 - val_loss: 0.0389 - val_accuracy: 0.9960\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0364 - accuracy: 0.9955 - val_loss: 0.0352 - val_accuracy: 0.9960\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0331 - accuracy: 0.9961 - val_loss: 0.0323 - val_accuracy: 0.9970\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0304 - accuracy: 0.9960 - val_loss: 0.0294 - val_accuracy: 0.9980\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0279 - accuracy: 0.9963 - val_loss: 0.0274 - val_accuracy: 0.9980\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0257 - accuracy: 0.9970 - val_loss: 0.0252 - val_accuracy: 0.9980\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0237 - accuracy: 0.9969 - val_loss: 0.0230 - val_accuracy: 0.9980\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0220 - accuracy: 0.9971 - val_loss: 0.0216 - val_accuracy: 0.9980\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0203 - accuracy: 0.9973 - val_loss: 0.0207 - val_accuracy: 0.9980\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 0.0193 - accuracy: 0.9975 - val_loss: 0.0186 - val_accuracy: 0.9980\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 19us/step - loss: 0.0177 - accuracy: 0.9974 - val_loss: 0.0176 - val_accuracy: 0.9980\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0166 - accuracy: 0.9977 - val_loss: 0.0168 - val_accuracy: 0.9980\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0156 - accuracy: 0.9980 - val_loss: 0.0154 - val_accuracy: 0.9980\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0146 - accuracy: 0.9976 - val_loss: 0.0144 - val_accuracy: 0.9980\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.0136 - val_accuracy: 0.9980\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0130 - accuracy: 0.9979 - val_loss: 0.0129 - val_accuracy: 0.9990\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0123 - accuracy: 0.9984 - val_loss: 0.0121 - val_accuracy: 0.9980\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0117 - accuracy: 0.9982 - val_loss: 0.0113 - val_accuracy: 0.9990\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0109 - val_accuracy: 0.9990\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0103 - accuracy: 0.9987 - val_loss: 0.0104 - val_accuracy: 0.9990\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.0097 - val_accuracy: 0.9990\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0093 - accuracy: 0.9988 - val_loss: 0.0091 - val_accuracy: 0.9990\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.0089 - accuracy: 0.9987 - val_loss: 0.0089 - val_accuracy: 0.9990\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.0082 - val_accuracy: 0.9990\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0080 - accuracy: 0.9988 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 16us/step - loss: 0.0076 - accuracy: 0.9988 - val_loss: 0.0078 - val_accuracy: 0.9990\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0070 - accuracy: 0.9992 - val_loss: 0.0067 - val_accuracy: 0.9990\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0066 - accuracy: 0.9991 - val_loss: 0.0063 - val_accuracy: 0.9990\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.0062 - val_accuracy: 0.9990\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0060 - accuracy: 0.9993 - val_loss: 0.0059 - val_accuracy: 0.9990\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0054 - accuracy: 0.9992 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0052 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9990\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 0.9990\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0051 - accuracy: 0.9992 - val_loss: 0.0048 - val_accuracy: 0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 0.0042 - val_accuracy: 0.9990\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0038 - accuracy: 0.9995 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9990\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 17us/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.7698e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 9.8382e-04 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 9.5434e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0010 - accuracy: 0.9999 - val_loss: 8.9826e-04 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 9.7534e-04 - accuracy: 0.9999 - val_loss: 8.4833e-04 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 9.1587e-04 - accuracy: 0.9999 - val_loss: 8.2658e-04 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 8.7624e-04 - accuracy: 1.0000 - val_loss: 7.8538e-04 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 8.3537e-04 - accuracy: 0.9999 - val_loss: 7.6871e-04 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 8.0459e-04 - accuracy: 1.0000 - val_loss: 7.0214e-04 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 7.6739e-04 - accuracy: 1.0000 - val_loss: 6.8134e-04 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 8.3724e-04 - accuracy: 0.9999 - val_loss: 6.9657e-04 - val_accuracy: 1.0000\n",
      "Baseline Error: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thsyu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "ax2.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['val_accuracy'])\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 595 405\n",
      "[[9.9829131e-01 1.7087196e-03]\n",
      " [9.9995887e-01 4.1117826e-05]\n",
      " [9.9999917e-01 8.5586095e-07]\n",
      " ...\n",
      " [1.0852546e-03 9.9891472e-01]\n",
      " [6.6444532e-05 9.9993360e-01]\n",
      " [2.3782770e-04 9.9976212e-01]]\n"
     ]
    }
   ],
   "source": [
    "_, _, new_X, new_y = generate_dataset(n_train=10, n_test=1000)\n",
    "new_pred_y = model.predict(new_X)\n",
    "#new_pred_y = model.evaluate(new_X, new_y, verbose=1)\n",
    "\n",
    "print (new_pred_y)#, np.argmax(new_pred_y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify and Regress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Keras loss function that conditionally creates a zero gradient: https://stackoverflow.com/questions/54031644/custom-keras-loss-function-that-conditionally-creates-a-zero-gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "def generate_dataset_wparams(n_train=10000, n_test=1000, frac_gauss=None):\n",
    "    # From desired training and test set sizes, determine the fraction that is Gaussian vs Lorentzian\n",
    "    \n",
    "    # Random fraction w/limits to avoid too many of any one kind\n",
    "    if frac_gauss is None:\n",
    "        fg_train = np.random.uniform(0.25, 0.75)\n",
    "        fg_test = np.random.uniform(0.25, 0.75)\n",
    "    \n",
    "    # User-identified fraction\n",
    "    elif frac_gauss < 1.0:\n",
    "        fg_train = fg_test = frac_gauss\n",
    "        \n",
    "    num_gauss_train = int(fg_train * n_train)\n",
    "    num_gauss_test =  int(fg_test * n_test)\n",
    "    \n",
    "    num_lorentz_train = n_train - num_gauss_train\n",
    "    num_lorentz_test = n_test - num_gauss_test\n",
    "    \n",
    "    print (num_gauss_train, num_lorentz_train, num_gauss_test, num_lorentz_test)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    X_train_gauss, y_train_gauss = make_gaussians(num_gauss_train)\n",
    "    X_test_gauss, y_test_gauss = make_gaussians(num_gauss_test)\n",
    "\n",
    "    X_train_lorentz, y_train_lorentz = make_lorentzians(num_lorentz_train)\n",
    "    X_test_lorentz, y_test_lorentz = make_lorentzians(num_lorentz_test)\n",
    "\n",
    "    # Combine Gaussian and Lorentzians\n",
    "    X_train = np.concatenate((X_train_gauss, X_train_lorentz))\n",
    "    y_train = np.concatenate((y_train_gauss, y_train_lorentz))\n",
    "    X_test = np.concatenate((X_test_gauss, X_test_lorentz))\n",
    "    y_test = np.concatenate((y_test_gauss, y_test_lorentz))\n",
    "    \n",
    "    # Want to classify the arrays too...?\n",
    "    # Classification\n",
    "    class_train = np.ones(n_train)\n",
    "    class_train[0:num_gauss_train] *= 0.0 # Gaussians are 0s\n",
    "    #class_train = np_utils.to_categorical(class_train)\n",
    "    \n",
    "    class_test = np.ones(n_test)\n",
    "    class_test[0:num_gauss_test] *= 0.0 # Gaussians are 0s\n",
    "    #class_test = np_utils.to_categorical(class_test)    \n",
    "\n",
    "    return X_train, y_train, X_test, y_test, class_train, class_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2959 7041 713 287\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X_train, y_train, X_test, y_test, class_train, class_test = generate_dataset_wparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Want a loss that returns loss = 0.0 if the class is a Lorentzian\n",
    "    loss = losses.mean_squared_error(y_true, y_pred)\n",
    "    # K.switch(condition, then expression, else expression)\n",
    "    # K.switch(if y_true is 0.)\n",
    "    pdb.set_trace()\n",
    "    \n",
    "    #### What is y_true here? ####\n",
    "    #### Want (if y_true is 0.) to only be the classification -- y_true[:,0]?\n",
    "    \n",
    "    return K.switch(K.flatten(K.equal(y_true, 0.)), loss, K.zeros_like(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "def baseline_model():\n",
    "    # Create model\n",
    "    verbose, epochs, batch_size = 1, 100, 16\n",
    "    input_shape = X_train.shape[1]\n",
    "\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "\n",
    "    x = Dense(10, activation='relu')(inputs)\n",
    "    x = Dense(5, activation='relu')(x)\n",
    "    \n",
    "    classify = Dense(1, activation='softmax', name='type')(x)\n",
    "    regress1 = Dense(1, activation='linear', name='param1')(x)\n",
    "    regress2 = Dense(1, activation='linear', name='param2')(x)\n",
    "    \n",
    "    # Create model with input layer and dense layers\n",
    "    model = Model(inputs=inputs, outputs=[classify, regress1, regress2])\n",
    "    model.compile(loss=['binary_crossentropy', 'mse', 'mse'], optimizer='rmsprop')#, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 90us/step - loss: 6.3775 - type_loss: 4.5375 - param1_loss: 0.3462 - param2_loss: 1.4920\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 4.9064 - type_loss: 4.5390 - param1_loss: 0.2404 - param2_loss: 0.1286\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 1s 70us/step - loss: 4.7029 - type_loss: 4.5360 - param1_loss: 0.0755 - param2_loss: 0.0902\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 4.6398 - type_loss: 4.5421 - param1_loss: 0.0323 - param2_loss: 0.0704\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.6204 - type_loss: 4.5314 - param1_loss: 0.0261 - param2_loss: 0.0572\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.6085 - type_loss: 4.5360 - param1_loss: 0.0232 - param2_loss: 0.0483\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5996 - type_loss: 4.5375 - param1_loss: 0.0208 - param2_loss: 0.0416\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5924 - type_loss: 4.5360 - param1_loss: 0.0185 - param2_loss: 0.0368\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5863 - type_loss: 4.5360 - param1_loss: 0.0166 - param2_loss: 0.0326\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 4.5818 - type_loss: 4.5406 - param1_loss: 0.0149 - param2_loss: 0.0297\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.5779 - type_loss: 4.5390 - param1_loss: 0.0135 - param2_loss: 0.0272\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5748 - type_loss: 4.5360 - param1_loss: 0.0124 - param2_loss: 0.0253\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.5720 - type_loss: 4.5375 - param1_loss: 0.0114 - param2_loss: 0.0234\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.5696 - type_loss: 4.5375 - param1_loss: 0.0108 - param2_loss: 0.0217\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.5675 - type_loss: 4.5375 - param1_loss: 0.0101 - param2_loss: 0.0202\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 1s 74us/step - loss: 4.5654 - type_loss: 4.5329 - param1_loss: 0.0097 - param2_loss: 0.0187\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5637 - type_loss: 4.5375 - param1_loss: 0.0092 - param2_loss: 0.0173\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 4.5619 - type_loss: 4.5421 - param1_loss: 0.0088 - param2_loss: 0.0159\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 4.5602 - type_loss: 4.5329 - param1_loss: 0.0085 - param2_loss: 0.0146\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5587 - type_loss: 4.5360 - param1_loss: 0.0082 - param2_loss: 0.0134\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5573 - type_loss: 4.5375 - param1_loss: 0.0078 - param2_loss: 0.0124\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 4.5562 - type_loss: 4.5375 - param1_loss: 0.0076 - param2_loss: 0.0115\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 4.5552 - type_loss: 4.5329 - param1_loss: 0.0073 - param2_loss: 0.0108\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 4.5542 - type_loss: 4.5375 - param1_loss: 0.0070 - param2_loss: 0.0102\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 4.5533 - type_loss: 4.5344 - param1_loss: 0.0067 - param2_loss: 0.0095\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 1s 84us/step - loss: 4.5526 - type_loss: 4.5375 - param1_loss: 0.0064 - param2_loss: 0.0090\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 1s 89us/step - loss: 4.5518 - type_loss: 4.5345 - param1_loss: 0.0061 - param2_loss: 0.0086\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5511 - type_loss: 4.5360 - param1_loss: 0.0058 - param2_loss: 0.0082\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5503 - type_loss: 4.5360 - param1_loss: 0.0054 - param2_loss: 0.0077\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 1s 74us/step - loss: 4.5496 - type_loss: 4.5344 - param1_loss: 0.0050 - param2_loss: 0.0075\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5487 - type_loss: 4.5344 - param1_loss: 0.0045 - param2_loss: 0.0071: 0s - loss: 4.5947 - type_loss: 4.5831 - param1_loss: 0.0046 - param2_loss: \n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 4.5478 - type_loss: 4.5344 - param1_loss: 0.0040 - param2_loss: 0.0067\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 1s 76us/step - loss: 4.5472 - type_loss: 4.5390 - param1_loss: 0.0037 - param2_loss: 0.0064\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5466 - type_loss: 4.5329 - param1_loss: 0.0034 - param2_loss: 0.0061\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.5461 - type_loss: 4.5375 - param1_loss: 0.0032 - param2_loss: 0.0059\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 4.5458 - type_loss: 4.5360 - param1_loss: 0.0030 - param2_loss: 0.0057\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.5455 - type_loss: 4.5375 - param1_loss: 0.0028 - param2_loss: 0.0056\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5452 - type_loss: 4.5390 - param1_loss: 0.0027 - param2_loss: 0.0054\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5449 - type_loss: 4.5344 - param1_loss: 0.0026 - param2_loss: 0.0053\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 4.5447 - type_loss: 4.5375 - param1_loss: 0.0024 - param2_loss: 0.0052\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.5445 - type_loss: 4.5406 - param1_loss: 0.0023 - param2_loss: 0.0051\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5443 - type_loss: 4.5406 - param1_loss: 0.0022 - param2_loss: 0.0050\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 1s 74us/step - loss: 4.5441 - type_loss: 4.5360 - param1_loss: 0.0021 - param2_loss: 0.0049\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 4.5440 - type_loss: 4.5344 - param1_loss: 0.0020 - param2_loss: 0.0049\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 1s 78us/step - loss: 4.5439 - type_loss: 4.5360 - param1_loss: 0.0019 - param2_loss: 0.0049\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 4.5438 - type_loss: 4.5360 - param1_loss: 0.0018 - param2_loss: 0.0048\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 4.5436 - type_loss: 4.5375 - param1_loss: 0.0017 - param2_loss: 0.0048\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5435 - type_loss: 4.5406 - param1_loss: 0.0017 - param2_loss: 0.0047: 0s - loss: 4.5219 - type_loss: 4.5156 - param1_loss: 0.0017 - param2_loss: 0.\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 4.5434 - type_loss: 4.5360 - param1_loss: 0.0016 - param2_loss: 0.0047\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 4.5433 - type_loss: 4.5360 - param1_loss: 0.0016 - param2_loss: 0.0046\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 4.5432 - type_loss: 4.5329 - param1_loss: 0.0015 - param2_loss: 0.0046\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 1s 70us/step - loss: 4.5431 - type_loss: 4.5390 - param1_loss: 0.0015 - param2_loss: 0.0046\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 77us/step - loss: 4.5431 - type_loss: 4.5390 - param1_loss: 0.0014 - param2_loss: 0.0046\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 1s 76us/step - loss: 4.5430 - type_loss: 4.5360 - param1_loss: 0.0014 - param2_loss: 0.0045\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 1s 76us/step - loss: 4.5430 - type_loss: 4.5436 - param1_loss: 0.0013 - param2_loss: 0.0046: 0s - loss: 4.1538 - type_loss: 4.1482 - param1_loss: 0.0014 - param2\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 1s 83us/step - loss: 4.5429 - type_loss: 4.5360 - param1_loss: 0.0013 - param2_loss: 0.0045\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 4.5429 - type_loss: 4.5329 - param1_loss: 0.0013 - param2_loss: 0.0045\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5428 - type_loss: 4.5375 - param1_loss: 0.0012 - param2_loss: 0.0045\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 4.5428 - type_loss: 4.5329 - param1_loss: 0.0012 - param2_loss: 0.0045\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 1s 80us/step - loss: 4.5427 - type_loss: 4.5314 - param1_loss: 0.0012 - param2_loss: 0.0044\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 1s 70us/step - loss: 4.5427 - type_loss: 4.5360 - param1_loss: 0.0012 - param2_loss: 0.0044\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 4.5426 - type_loss: 4.5360 - param1_loss: 0.0011 - param2_loss: 0.0044\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.5426 - type_loss: 4.5329 - param1_loss: 0.0011 - param2_loss: 0.0044\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.5426 - type_loss: 4.5390 - param1_loss: 0.0011 - param2_loss: 0.0044\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 4.5425 - type_loss: 4.5360 - param1_loss: 0.0011 - param2_loss: 0.0043\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 1s 70us/step - loss: 4.5425 - type_loss: 4.5390 - param1_loss: 0.0011 - param2_loss: 0.0043\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 4.5424 - type_loss: 4.5390 - param1_loss: 0.0011 - param2_loss: 0.0043\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5425 - type_loss: 4.5344 - param1_loss: 0.0010 - param2_loss: 0.0043\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 4.5424 - type_loss: 4.5360 - param1_loss: 0.0010 - param2_loss: 0.0043\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 4.5424 - type_loss: 4.5360 - param1_loss: 0.0010 - param2_loss: 0.0043\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5423 - type_loss: 4.5344 - param1_loss: 9.9558e-04 - param2_loss: 0.0042\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 4.5423 - type_loss: 4.5360 - param1_loss: 0.0010 - param2_loss: 0.0042\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 4.5423 - type_loss: 4.5390 - param1_loss: 0.0010 - param2_loss: 0.0042\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 1s 74us/step - loss: 4.5424 - type_loss: 4.5360 - param1_loss: 9.8287e-04 - param2_loss: 0.0043\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 4.5423 - type_loss: 4.5329 - param1_loss: 9.8541e-04 - param2_loss: 0.0042\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 4.5422 - type_loss: 4.5406 - param1_loss: 9.7266e-04 - param2_loss: 0.0042\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 4.5422 - type_loss: 4.5344 - param1_loss: 9.5101e-04 - param2_loss: 0.0041\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 4.5422 - type_loss: 4.5406 - param1_loss: 9.4200e-04 - param2_loss: 0.0041\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 4.5422 - type_loss: 4.5375 - param1_loss: 9.3344e-04 - param2_loss: 0.0041\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5422 - type_loss: 4.5421 - param1_loss: 9.2338e-04 - param2_loss: 0.0041\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.5421 - type_loss: 4.5436 - param1_loss: 9.3893e-04 - param2_loss: 0.0041\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5421 - type_loss: 4.5329 - param1_loss: 8.9817e-04 - param2_loss: 0.0041\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 4.5421 - type_loss: 4.5375 - param1_loss: 9.0574e-04 - param2_loss: 0.0041\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 4.5420 - type_loss: 4.5360 - param1_loss: 9.0158e-04 - param2_loss: 0.0040\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 4.5421 - type_loss: 4.5329 - param1_loss: 9.1493e-04 - param2_loss: 0.0041\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 4.5421 - type_loss: 4.5344 - param1_loss: 8.9890e-04 - param2_loss: 0.0041\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 4.5420 - type_loss: 4.5344 - param1_loss: 8.8898e-04 - param2_loss: 0.0040\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5421 - type_loss: 4.5344 - param1_loss: 8.7381e-04 - param2_loss: 0.0041\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 4.5420 - type_loss: 4.5375 - param1_loss: 8.6332e-04 - param2_loss: 0.0040\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 4.5420 - type_loss: 4.5375 - param1_loss: 8.6918e-04 - param2_loss: 0.0040\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 4.5419 - type_loss: 4.5390 - param1_loss: 8.6436e-04 - param2_loss: 0.0040\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 4.5420 - type_loss: 4.5375 - param1_loss: 8.5108e-04 - param2_loss: 0.0040\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5419 - type_loss: 4.5406 - param1_loss: 8.4490e-04 - param2_loss: 0.0040\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 4.5419 - type_loss: 4.5360 - param1_loss: 8.4309e-04 - param2_loss: 0.0040\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 4.5419 - type_loss: 4.5360 - param1_loss: 8.5615e-04 - param2_loss: 0.0040\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5419 - type_loss: 4.5406 - param1_loss: 8.3744e-04 - param2_loss: 0.0039\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 4.5419 - type_loss: 4.5344 - param1_loss: 8.5907e-04 - param2_loss: 0.0039\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 4.5419 - type_loss: 4.5375 - param1_loss: 8.2467e-04 - param2_loss: 0.0040\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 4.5419 - type_loss: 4.5375 - param1_loss: 8.3246e-04 - param2_loss: 0.0039\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 4.5419 - type_loss: 4.5421 - param1_loss: 8.2695e-04 - param2_loss: 0.0040\n",
      "Baseline Error: -961.79%\n"
     ]
    }
   ],
   "source": [
    "verbose, epochs, batch_size = 1, 100, 16\n",
    "\n",
    "# Build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, [np.vstack(class_train), y_train[:,0], y_train[:,1]], epochs=epochs, verbose=verbose)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, [np.vstack(class_test), y_test[:,0], y_test[:,1]], verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 6 4\n"
     ]
    }
   ],
   "source": [
    "_, _, new_X, new_y, new_ctr, new_cte = generate_dataset_wparams(n_train=10, n_test=10)\n",
    "new_pred_y = model.predict(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following classification+regression (gender+age) problem from online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify and regress at the same time: https://stats.stackexchange.com/questions/77330/classify-and-regress-at-the-same-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')\n",
    "adult.columns = ['age', 'workclass', 'fnlwgt', 'edu', 'edu_num', 'marital_status',\n",
    "                 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "                 'hours_per_week', 'native_country', 'income']\n",
    "adult[\"sex\"] = adult[\"sex\"].astype('category').cat.codes\n",
    "adult[\"workclass\"] = adult[\"workclass\"].astype('category').cat.codes\n",
    "adult[\"marital_status\"] = adult[\"marital_status\"].astype('category').cat.codes\n",
    "adult[\"race\"] = adult[\"race\"].astype('category').cat.codes\n",
    "adult[\"occupation\"] = adult[\"occupation\"].astype('category').cat.codes\n",
    "adult[\"native_country\"] = adult[\"native_country\"].astype('category').cat.codes\n",
    "\n",
    "target_bin = adult[['sex']].values.astype('float32') # 0 is female?\n",
    "target_num = adult[['age']].values.astype('float32')\n",
    "X = adult[['workclass', 'edu_num', 'marital_status', 'occupation', 'race', 'capital_gain',\n",
    "           'capital_loss', 'hours_per_week', 'native_country']].values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>edu</th>\n",
       "      <th>edu_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Husband</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Husband</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Wife</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Wife</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt         edu  edu_num  marital_status  occupation  \\\n",
       "0   50          6   83311   Bachelors       13               2           4   \n",
       "1   38          4  215646     HS-grad        9               0           6   \n",
       "2   53          4  234721        11th        7               2           6   \n",
       "3   28          4  338409   Bachelors       13               2          10   \n",
       "4   37          4  284582     Masters       14               2           4   \n",
       "\n",
       "     relationship  race  sex  capital_gain  capital_loss  hours_per_week  \\\n",
       "0         Husband     4    1             0             0              13   \n",
       "1   Not-in-family     4    1             0             0              40   \n",
       "2         Husband     2    1             0             0              40   \n",
       "3            Wife     2    0             0             0              40   \n",
       "4            Wife     4    0             0             0              40   \n",
       "\n",
       "   native_country  income  \n",
       "0              39   <=50K  \n",
       "1              39   <=50K  \n",
       "2              39   <=50K  \n",
       "3               5   <=50K  \n",
       "4              39   <=50K  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Shared-Hidden-Layer (Dense)     (None, 64)           640         Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           256         Shared-Hidden-Layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Output-Bin (Dense)              (None, 1)            65          batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Output-Num (Dense)              (None, 1)            65          batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,026\n",
      "Trainable params: 898\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "inputs = Input(shape=(X.shape[1],), name='Input')\n",
    "\n",
    "hidden = Dense(64, name='Shared-Hidden-Layer', activation='relu')(inputs)\n",
    "hidden = BatchNormalization()(hidden)\n",
    "out_bin = Dense(1, name='Output-Bin', activation='sigmoid')(hidden)\n",
    "out_num = Dense(1, name='Output-Num', activation='linear')(hidden)\n",
    "\n",
    "model = Model(inputs, [out_bin, out_num])\n",
    "model.compile(optimizer=Adam(0.10), loss=['binary_crossentropy', 'mean_squared_error'])\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26048 samples, validate on 6512 samples\n",
      "Epoch 1/100\n",
      "26048/26048 [==============================] - 0s 16us/step - loss: 792.0078 - Output-Bin_loss: 0.7100 - Output-Num_loss: 784.0192 - val_loss: 337.0345 - val_Output-Bin_loss: 1.2921 - val_Output-Num_loss: 343.6743\n",
      "Epoch 2/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 245.4264 - Output-Bin_loss: 0.6511 - Output-Num_loss: 243.9992 - val_loss: 187.5228 - val_Output-Bin_loss: 1.3022 - val_Output-Num_loss: 193.0143\n",
      "Epoch 3/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 185.0638 - Output-Bin_loss: 0.6269 - Output-Num_loss: 184.1155 - val_loss: 185.6181 - val_Output-Bin_loss: 0.8574 - val_Output-Num_loss: 191.9695\n",
      "Epoch 4/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 176.0321 - Output-Bin_loss: 0.6215 - Output-Num_loss: 175.4167 - val_loss: 227.1078 - val_Output-Bin_loss: 0.7832 - val_Output-Num_loss: 233.2839\n",
      "Epoch 5/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 168.0636 - Output-Bin_loss: 0.5967 - Output-Num_loss: 167.2844 - val_loss: 255.8982 - val_Output-Bin_loss: 0.9220 - val_Output-Num_loss: 264.0811\n",
      "Epoch 6/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 167.4300 - Output-Bin_loss: 0.6124 - Output-Num_loss: 166.9167 - val_loss: 374.1068 - val_Output-Bin_loss: 0.9103 - val_Output-Num_loss: 384.3699\n",
      "Epoch 7/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 165.5672 - Output-Bin_loss: 0.6218 - Output-Num_loss: 164.9261 - val_loss: 316.9416 - val_Output-Bin_loss: 0.7310 - val_Output-Num_loss: 325.2412\n",
      "Epoch 8/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 163.8335 - Output-Bin_loss: 0.6196 - Output-Num_loss: 163.2585 - val_loss: 232.3904 - val_Output-Bin_loss: 0.6936 - val_Output-Num_loss: 238.9377\n",
      "Epoch 9/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 162.7843 - Output-Bin_loss: 0.6206 - Output-Num_loss: 161.9867 - val_loss: 198.1516 - val_Output-Bin_loss: 0.6481 - val_Output-Num_loss: 203.4908\n",
      "Epoch 10/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 161.5049 - Output-Bin_loss: 0.6228 - Output-Num_loss: 160.8591 - val_loss: 187.7134 - val_Output-Bin_loss: 0.8318 - val_Output-Num_loss: 191.4128\n",
      "Epoch 11/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 160.2518 - Output-Bin_loss: 0.6323 - Output-Num_loss: 159.3129 - val_loss: 193.5313 - val_Output-Bin_loss: 0.9521 - val_Output-Num_loss: 195.5277\n",
      "Epoch 12/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 158.3766 - Output-Bin_loss: 0.6548 - Output-Num_loss: 157.6771 - val_loss: 209.8061 - val_Output-Bin_loss: 1.2187 - val_Output-Num_loss: 209.5838\n",
      "Epoch 13/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 156.6817 - Output-Bin_loss: 0.6313 - Output-Num_loss: 155.8247 - val_loss: 229.6917 - val_Output-Bin_loss: 1.0848 - val_Output-Num_loss: 229.1620\n",
      "Epoch 14/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 154.1076 - Output-Bin_loss: 0.6122 - Output-Num_loss: 153.3881 - val_loss: 258.3386 - val_Output-Bin_loss: 1.1450 - val_Output-Num_loss: 256.0812\n",
      "Epoch 15/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 153.0152 - Output-Bin_loss: 0.6111 - Output-Num_loss: 152.5252 - val_loss: 287.1389 - val_Output-Bin_loss: 1.0739 - val_Output-Num_loss: 284.0323\n",
      "Epoch 16/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 149.4533 - Output-Bin_loss: 0.6216 - Output-Num_loss: 148.9786 - val_loss: 339.1246 - val_Output-Bin_loss: 1.1679 - val_Output-Num_loss: 334.2658\n",
      "Epoch 17/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 148.5815 - Output-Bin_loss: 0.6106 - Output-Num_loss: 148.0384 - val_loss: 355.1085 - val_Output-Bin_loss: 1.3079 - val_Output-Num_loss: 351.0561\n",
      "Epoch 18/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 162.7517 - Output-Bin_loss: 0.6311 - Output-Num_loss: 162.2103 - val_loss: 382.6477 - val_Output-Bin_loss: 1.0095 - val_Output-Num_loss: 383.9368\n",
      "Epoch 19/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 149.1225 - Output-Bin_loss: 0.6283 - Output-Num_loss: 148.4652 - val_loss: 340.9029 - val_Output-Bin_loss: 0.9454 - val_Output-Num_loss: 341.7766\n",
      "Epoch 20/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 146.8347 - Output-Bin_loss: 0.6137 - Output-Num_loss: 146.0402 - val_loss: 331.0893 - val_Output-Bin_loss: 1.0864 - val_Output-Num_loss: 328.6998\n",
      "Epoch 21/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 145.3802 - Output-Bin_loss: 0.6205 - Output-Num_loss: 144.6075 - val_loss: 396.6301 - val_Output-Bin_loss: 0.8147 - val_Output-Num_loss: 391.1310\n",
      "Epoch 22/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 142.7448 - Output-Bin_loss: 0.6191 - Output-Num_loss: 141.9597 - val_loss: 381.7171 - val_Output-Bin_loss: 0.9390 - val_Output-Num_loss: 377.5813\n",
      "Epoch 23/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 142.1093 - Output-Bin_loss: 0.6126 - Output-Num_loss: 141.3329 - val_loss: 441.6301 - val_Output-Bin_loss: 0.8917 - val_Output-Num_loss: 433.7558\n",
      "Epoch 24/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 143.8271 - Output-Bin_loss: 0.6028 - Output-Num_loss: 143.3421 - val_loss: 427.1707 - val_Output-Bin_loss: 0.8225 - val_Output-Num_loss: 419.6161\n",
      "Epoch 25/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 143.9913 - Output-Bin_loss: 0.5964 - Output-Num_loss: 143.3983 - val_loss: 387.1092 - val_Output-Bin_loss: 0.7327 - val_Output-Num_loss: 380.9442\n",
      "Epoch 26/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 143.0671 - Output-Bin_loss: 0.5984 - Output-Num_loss: 142.3010 - val_loss: 348.2580 - val_Output-Bin_loss: 0.7977 - val_Output-Num_loss: 343.7382\n",
      "Epoch 27/100\n",
      "26048/26048 [==============================] - 0s 5us/step - loss: 140.0514 - Output-Bin_loss: 0.5922 - Output-Num_loss: 139.4489 - val_loss: 338.7597 - val_Output-Bin_loss: 0.8284 - val_Output-Num_loss: 334.3005\n",
      "Epoch 28/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 138.2609 - Output-Bin_loss: 0.5904 - Output-Num_loss: 137.6942 - val_loss: 346.5860 - val_Output-Bin_loss: 0.7797 - val_Output-Num_loss: 341.7026\n",
      "Epoch 29/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 138.4173 - Output-Bin_loss: 0.5912 - Output-Num_loss: 137.8980 - val_loss: 318.7735 - val_Output-Bin_loss: 0.9084 - val_Output-Num_loss: 316.6859\n",
      "Epoch 30/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 140.2955 - Output-Bin_loss: 0.6048 - Output-Num_loss: 139.7267 - val_loss: 321.6436 - val_Output-Bin_loss: 0.7510 - val_Output-Num_loss: 318.9016\n",
      "Epoch 31/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 137.9385 - Output-Bin_loss: 0.6000 - Output-Num_loss: 137.3702 - val_loss: 316.0174 - val_Output-Bin_loss: 0.7363 - val_Output-Num_loss: 313.0154\n",
      "Epoch 32/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 137.9510 - Output-Bin_loss: 0.5936 - Output-Num_loss: 137.4552 - val_loss: 290.7678 - val_Output-Bin_loss: 0.7421 - val_Output-Num_loss: 288.6330\n",
      "Epoch 33/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 137.3166 - Output-Bin_loss: 0.5926 - Output-Num_loss: 136.7566 - val_loss: 286.0674 - val_Output-Bin_loss: 0.7090 - val_Output-Num_loss: 285.0881\n",
      "Epoch 34/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 136.1612 - Output-Bin_loss: 0.5983 - Output-Num_loss: 135.6486 - val_loss: 276.2835 - val_Output-Bin_loss: 0.7096 - val_Output-Num_loss: 276.0150\n",
      "Epoch 35/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 135.6675 - Output-Bin_loss: 0.5862 - Output-Num_loss: 135.0446 - val_loss: 266.7756 - val_Output-Bin_loss: 0.6809 - val_Output-Num_loss: 264.9442\n",
      "Epoch 36/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 136.0736 - Output-Bin_loss: 0.5853 - Output-Num_loss: 135.3589 - val_loss: 241.1250 - val_Output-Bin_loss: 0.7013 - val_Output-Num_loss: 241.1715\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 3us/step - loss: 135.9178 - Output-Bin_loss: 0.5974 - Output-Num_loss: 135.4676 - val_loss: 251.3296 - val_Output-Bin_loss: 0.6769 - val_Output-Num_loss: 251.9504\n",
      "Epoch 38/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 135.6060 - Output-Bin_loss: 0.5912 - Output-Num_loss: 135.0175 - val_loss: 217.5473 - val_Output-Bin_loss: 0.7504 - val_Output-Num_loss: 217.4754\n",
      "Epoch 39/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 133.8596 - Output-Bin_loss: 0.5974 - Output-Num_loss: 133.2442 - val_loss: 223.0804 - val_Output-Bin_loss: 0.6556 - val_Output-Num_loss: 223.0572\n",
      "Epoch 40/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 136.0385 - Output-Bin_loss: 0.5907 - Output-Num_loss: 135.6732 - val_loss: 189.3207 - val_Output-Bin_loss: 0.7227 - val_Output-Num_loss: 190.4678\n",
      "Epoch 41/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 136.7632 - Output-Bin_loss: 0.5901 - Output-Num_loss: 136.3353 - val_loss: 217.7167 - val_Output-Bin_loss: 0.7143 - val_Output-Num_loss: 219.2574\n",
      "Epoch 42/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 133.8506 - Output-Bin_loss: 0.5817 - Output-Num_loss: 133.3707 - val_loss: 220.7345 - val_Output-Bin_loss: 0.6894 - val_Output-Num_loss: 220.5385\n",
      "Epoch 43/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 132.9105 - Output-Bin_loss: 0.5784 - Output-Num_loss: 132.2172 - val_loss: 210.5503 - val_Output-Bin_loss: 0.6327 - val_Output-Num_loss: 210.7341\n",
      "Epoch 44/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 132.3639 - Output-Bin_loss: 0.5781 - Output-Num_loss: 131.9493 - val_loss: 187.0685 - val_Output-Bin_loss: 0.6821 - val_Output-Num_loss: 188.9426\n",
      "Epoch 45/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 133.8628 - Output-Bin_loss: 0.5731 - Output-Num_loss: 133.3668 - val_loss: 181.5877 - val_Output-Bin_loss: 0.6516 - val_Output-Num_loss: 183.8340\n",
      "Epoch 46/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 133.8875 - Output-Bin_loss: 0.5748 - Output-Num_loss: 133.3573 - val_loss: 181.2034 - val_Output-Bin_loss: 0.6645 - val_Output-Num_loss: 183.1131\n",
      "Epoch 47/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 132.2064 - Output-Bin_loss: 0.5725 - Output-Num_loss: 131.7487 - val_loss: 174.2998 - val_Output-Bin_loss: 0.6765 - val_Output-Num_loss: 177.0039\n",
      "Epoch 48/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 133.4405 - Output-Bin_loss: 0.5740 - Output-Num_loss: 132.9207 - val_loss: 182.2906 - val_Output-Bin_loss: 0.6288 - val_Output-Num_loss: 185.5283\n",
      "Epoch 49/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 134.8013 - Output-Bin_loss: 0.5685 - Output-Num_loss: 134.3847 - val_loss: 193.4509 - val_Output-Bin_loss: 0.6488 - val_Output-Num_loss: 196.2138\n",
      "Epoch 50/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 133.0897 - Output-Bin_loss: 0.5676 - Output-Num_loss: 132.2819 - val_loss: 181.7482 - val_Output-Bin_loss: 0.6475 - val_Output-Num_loss: 184.5582\n",
      "Epoch 51/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 133.0909 - Output-Bin_loss: 0.5630 - Output-Num_loss: 132.5404 - val_loss: 189.9208 - val_Output-Bin_loss: 0.6207 - val_Output-Num_loss: 191.0309\n",
      "Epoch 52/100\n",
      "26048/26048 [==============================] - 0s 5us/step - loss: 134.1704 - Output-Bin_loss: 0.5692 - Output-Num_loss: 133.2243 - val_loss: 215.4818 - val_Output-Bin_loss: 0.6150 - val_Output-Num_loss: 218.8621\n",
      "Epoch 53/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 132.9994 - Output-Bin_loss: 0.5649 - Output-Num_loss: 132.4205 - val_loss: 200.8044 - val_Output-Bin_loss: 0.6387 - val_Output-Num_loss: 203.3189\n",
      "Epoch 54/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 133.0244 - Output-Bin_loss: 0.5637 - Output-Num_loss: 132.4376 - val_loss: 181.4590 - val_Output-Bin_loss: 0.6850 - val_Output-Num_loss: 183.2879\n",
      "Epoch 55/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 137.3524 - Output-Bin_loss: 0.5667 - Output-Num_loss: 137.2415 - val_loss: 275.5653 - val_Output-Bin_loss: 0.6256 - val_Output-Num_loss: 273.2214\n",
      "Epoch 56/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 141.1716 - Output-Bin_loss: 0.5715 - Output-Num_loss: 140.4727 - val_loss: 170.3565 - val_Output-Bin_loss: 0.6292 - val_Output-Num_loss: 173.9203\n",
      "Epoch 57/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 134.5864 - Output-Bin_loss: 0.5685 - Output-Num_loss: 133.9700 - val_loss: 174.3251 - val_Output-Bin_loss: 0.6186 - val_Output-Num_loss: 177.1208\n",
      "Epoch 58/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 132.6827 - Output-Bin_loss: 0.5605 - Output-Num_loss: 132.0958 - val_loss: 163.0009 - val_Output-Bin_loss: 0.6529 - val_Output-Num_loss: 166.4369\n",
      "Epoch 59/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 131.1952 - Output-Bin_loss: 0.5602 - Output-Num_loss: 130.6891 - val_loss: 165.1145 - val_Output-Bin_loss: 0.6098 - val_Output-Num_loss: 168.6544\n",
      "Epoch 60/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 130.7616 - Output-Bin_loss: 0.5608 - Output-Num_loss: 129.9750 - val_loss: 172.5458 - val_Output-Bin_loss: 0.5915 - val_Output-Num_loss: 175.1480\n",
      "Epoch 61/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 131.3610 - Output-Bin_loss: 0.5568 - Output-Num_loss: 130.9256 - val_loss: 215.4298 - val_Output-Bin_loss: 0.6111 - val_Output-Num_loss: 219.3419\n",
      "Epoch 62/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 130.7452 - Output-Bin_loss: 0.5552 - Output-Num_loss: 130.1200 - val_loss: 173.6170 - val_Output-Bin_loss: 0.7097 - val_Output-Num_loss: 176.7770\n",
      "Epoch 63/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 130.3563 - Output-Bin_loss: 0.5496 - Output-Num_loss: 129.8649 - val_loss: 182.5078 - val_Output-Bin_loss: 0.5820 - val_Output-Num_loss: 185.8529\n",
      "Epoch 64/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 126.1523 - Output-Bin_loss: 0.5316 - Output-Num_loss: 125.5898 - val_loss: 207.4713 - val_Output-Bin_loss: 0.5463 - val_Output-Num_loss: 212.4507\n",
      "Epoch 65/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 126.2678 - Output-Bin_loss: 0.5316 - Output-Num_loss: 125.7096 - val_loss: 166.9497 - val_Output-Bin_loss: 0.5570 - val_Output-Num_loss: 170.6743\n",
      "Epoch 66/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 125.4573 - Output-Bin_loss: 0.5428 - Output-Num_loss: 124.9718 - val_loss: 177.4779 - val_Output-Bin_loss: 0.5567 - val_Output-Num_loss: 178.7291\n",
      "Epoch 67/100\n",
      "26048/26048 [==============================] - 0s 6us/step - loss: 129.0941 - Output-Bin_loss: 0.5337 - Output-Num_loss: 128.4197 - val_loss: 141.0224 - val_Output-Bin_loss: 0.5498 - val_Output-Num_loss: 143.8703\n",
      "Epoch 68/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 125.4284 - Output-Bin_loss: 0.5384 - Output-Num_loss: 124.9025 - val_loss: 157.9542 - val_Output-Bin_loss: 0.5409 - val_Output-Num_loss: 161.4910\n",
      "Epoch 69/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 123.9600 - Output-Bin_loss: 0.5355 - Output-Num_loss: 123.5383 - val_loss: 160.8055 - val_Output-Bin_loss: 0.5568 - val_Output-Num_loss: 164.6767\n",
      "Epoch 70/100\n",
      "26048/26048 [==============================] - 0s 5us/step - loss: 123.1829 - Output-Bin_loss: 0.5319 - Output-Num_loss: 122.7739 - val_loss: 148.5532 - val_Output-Bin_loss: 0.5491 - val_Output-Num_loss: 152.1308\n",
      "Epoch 71/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 124.1649 - Output-Bin_loss: 0.5454 - Output-Num_loss: 123.6756 - val_loss: 170.7882 - val_Output-Bin_loss: 0.5766 - val_Output-Num_loss: 173.2792\n",
      "Epoch 72/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 122.7586 - Output-Bin_loss: 0.5550 - Output-Num_loss: 122.2983 - val_loss: 143.4497 - val_Output-Bin_loss: 0.6297 - val_Output-Num_loss: 145.3967\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048/26048 [==============================] - 0s 4us/step - loss: 125.9296 - Output-Bin_loss: 0.5581 - Output-Num_loss: 125.3942 - val_loss: 148.5918 - val_Output-Bin_loss: 0.5581 - val_Output-Num_loss: 151.5113\n",
      "Epoch 74/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 125.0182 - Output-Bin_loss: 0.5498 - Output-Num_loss: 124.5716 - val_loss: 139.2861 - val_Output-Bin_loss: 0.5322 - val_Output-Num_loss: 142.2808\n",
      "Epoch 75/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 125.7497 - Output-Bin_loss: 0.5415 - Output-Num_loss: 125.0949 - val_loss: 167.6484 - val_Output-Bin_loss: 0.5306 - val_Output-Num_loss: 170.9880\n",
      "Epoch 76/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 124.1314 - Output-Bin_loss: 0.5416 - Output-Num_loss: 123.6406 - val_loss: 144.7037 - val_Output-Bin_loss: 0.5644 - val_Output-Num_loss: 145.9820\n",
      "Epoch 77/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 127.0318 - Output-Bin_loss: 0.5302 - Output-Num_loss: 126.5534 - val_loss: 137.8854 - val_Output-Bin_loss: 0.5372 - val_Output-Num_loss: 140.2510\n",
      "Epoch 78/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 124.7177 - Output-Bin_loss: 0.5345 - Output-Num_loss: 124.1733 - val_loss: 149.3624 - val_Output-Bin_loss: 0.5477 - val_Output-Num_loss: 152.3756\n",
      "Epoch 79/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 122.8995 - Output-Bin_loss: 0.5356 - Output-Num_loss: 122.3762 - val_loss: 152.4722 - val_Output-Bin_loss: 0.5543 - val_Output-Num_loss: 155.6562\n",
      "Epoch 80/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 124.6262 - Output-Bin_loss: 0.5382 - Output-Num_loss: 124.2053 - val_loss: 153.6321 - val_Output-Bin_loss: 0.5764 - val_Output-Num_loss: 157.8427\n",
      "Epoch 81/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 124.5962 - Output-Bin_loss: 0.5326 - Output-Num_loss: 123.9717 - val_loss: 167.0500 - val_Output-Bin_loss: 0.5519 - val_Output-Num_loss: 171.7027\n",
      "Epoch 82/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 123.3712 - Output-Bin_loss: 0.5409 - Output-Num_loss: 122.6067 - val_loss: 161.9595 - val_Output-Bin_loss: 0.5329 - val_Output-Num_loss: 165.7500\n",
      "Epoch 83/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 123.4674 - Output-Bin_loss: 0.5344 - Output-Num_loss: 122.9747 - val_loss: 140.2790 - val_Output-Bin_loss: 0.5762 - val_Output-Num_loss: 142.3292\n",
      "Epoch 84/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 122.7846 - Output-Bin_loss: 0.5347 - Output-Num_loss: 122.2055 - val_loss: 139.2049 - val_Output-Bin_loss: 0.5315 - val_Output-Num_loss: 141.4978\n",
      "Epoch 85/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 122.1774 - Output-Bin_loss: 0.5370 - Output-Num_loss: 121.5694 - val_loss: 133.5719 - val_Output-Bin_loss: 0.5291 - val_Output-Num_loss: 135.5918\n",
      "Epoch 86/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.6059 - Output-Bin_loss: 0.5373 - Output-Num_loss: 120.8584 - val_loss: 139.4155 - val_Output-Bin_loss: 0.5647 - val_Output-Num_loss: 141.8777\n",
      "Epoch 87/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.7317 - Output-Bin_loss: 0.5392 - Output-Num_loss: 121.4065 - val_loss: 147.8179 - val_Output-Bin_loss: 0.5536 - val_Output-Num_loss: 149.3197\n",
      "Epoch 88/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 123.3178 - Output-Bin_loss: 0.5348 - Output-Num_loss: 122.5856 - val_loss: 154.8267 - val_Output-Bin_loss: 0.5427 - val_Output-Num_loss: 157.4338\n",
      "Epoch 89/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 122.3653 - Output-Bin_loss: 0.5456 - Output-Num_loss: 121.8610 - val_loss: 146.3777 - val_Output-Bin_loss: 0.5295 - val_Output-Num_loss: 147.7763\n",
      "Epoch 90/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 122.0571 - Output-Bin_loss: 0.5396 - Output-Num_loss: 121.5361 - val_loss: 143.5406 - val_Output-Bin_loss: 0.5376 - val_Output-Num_loss: 145.3737\n",
      "Epoch 91/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 121.5804 - Output-Bin_loss: 0.5330 - Output-Num_loss: 121.0427 - val_loss: 134.5981 - val_Output-Bin_loss: 0.5291 - val_Output-Num_loss: 136.5269\n",
      "Epoch 92/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.5188 - Output-Bin_loss: 0.5353 - Output-Num_loss: 120.7953 - val_loss: 139.2029 - val_Output-Bin_loss: 0.5376 - val_Output-Num_loss: 140.5506\n",
      "Epoch 93/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.9544 - Output-Bin_loss: 0.5373 - Output-Num_loss: 121.2342 - val_loss: 144.7230 - val_Output-Bin_loss: 0.5854 - val_Output-Num_loss: 145.6385\n",
      "Epoch 94/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.9305 - Output-Bin_loss: 0.5468 - Output-Num_loss: 121.4786 - val_loss: 139.6978 - val_Output-Bin_loss: 0.5850 - val_Output-Num_loss: 140.8846\n",
      "Epoch 95/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 123.7608 - Output-Bin_loss: 0.5468 - Output-Num_loss: 122.9932 - val_loss: 132.1758 - val_Output-Bin_loss: 0.5425 - val_Output-Num_loss: 133.2132\n",
      "Epoch 96/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.7970 - Output-Bin_loss: 0.5404 - Output-Num_loss: 121.2169 - val_loss: 127.7380 - val_Output-Bin_loss: 0.5378 - val_Output-Num_loss: 128.7641\n",
      "Epoch 97/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 121.7955 - Output-Bin_loss: 0.5432 - Output-Num_loss: 121.3236 - val_loss: 130.6697 - val_Output-Bin_loss: 0.5312 - val_Output-Num_loss: 131.4649\n",
      "Epoch 98/100\n",
      "26048/26048 [==============================] - 0s 3us/step - loss: 122.7756 - Output-Bin_loss: 0.5335 - Output-Num_loss: 122.3650 - val_loss: 131.2730 - val_Output-Bin_loss: 0.5564 - val_Output-Num_loss: 132.8917\n",
      "Epoch 99/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 122.7466 - Output-Bin_loss: 0.5484 - Output-Num_loss: 122.1589 - val_loss: 129.2907 - val_Output-Bin_loss: 0.5364 - val_Output-Num_loss: 130.5848\n",
      "Epoch 100/100\n",
      "26048/26048 [==============================] - 0s 4us/step - loss: 121.0762 - Output-Bin_loss: 0.5427 - Output-Num_loss: 120.3811 - val_loss: 126.0205 - val_Output-Bin_loss: 0.5268 - val_Output-Num_loss: 127.0406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x63a7b2b38>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, [target_bin, target_num], validation_split=.20, epochs=100, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "gend = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction incorrectly gendered: 26.851965601965603\n"
     ]
    }
   ],
   "source": [
    "print ('Fraction incorrectly gendered:', 100*len(np.where(np.around(gend[0].flatten()) - target_bin.flatten() != 0.)[0]) / len(target_bin.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
