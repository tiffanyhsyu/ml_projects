{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pdb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish Gaussians from Lorentzians, and predicting the mean, standard deviation if Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    #pdb.set_trace()\n",
    "    # Make 1000 new Gaussians to apply the model to\n",
    "    predX, predy = make_gaussians(1000)\n",
    "    \n",
    "    # Apply the model to get predicted means and sigmas of the Gaussians\n",
    "    pmeans, psigs = model.predict(predX, batch_size=None, verbose=0)\n",
    "    \n",
    "    # Check distribution of difference between true and predicted means, sigmas\n",
    "    plt.subplot(211)\n",
    "    _, _, _ = plt.hist(predy[:, 0] - pmeans.flatten(), bins=30)\n",
    "    plt.subplot(212)\n",
    "    _, _, _ = plt.hist(predy[:, 1] - psigs.flatten(), bins=30)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Check the relation between true and predicted means, sigmas\n",
    "    oto_means = np.linspace(-1., 1., 32) # one-to-one relation for means\n",
    "    oto_sigmas = np.linspace(0.25, 4.0, 32) # one-to-one relation for sigmas\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    # Plot the true y's and predicted y's from the NN model\n",
    "    plt.plot(predy[:, 0], pmeans.flatten(), marker='.')\n",
    "    # Plot the 1-to-1 line\n",
    "    plt.plot(oto_means, oto_means, color='black', ls='--')\n",
    "    plt.xlim(-1.0, 1.0)\n",
    "    plt.ylim(-1.0, 1.0)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(r'$\\mu$')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(predy[:, 1], psigs.flatten(), marker='.')\n",
    "    plt.plot(oto_sigmas, oto_sigmas, color='black', ls='--')\n",
    "    plt.xlim(0.25, 4.0)\n",
    "    plt.ylim(0.25, 4.0)\n",
    "    plt.xlabel('True value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title(r'$\\sigma$')\n",
    "    \n",
    "    #plt.savefig('gaussian_characteristics.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Gaussian given its mean and standard deviation\n",
    "def gaussian(x_vals, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x_vals - mu)/sigma)**2) / (sigma * np.sqrt(2*np.pi))\n",
    "\n",
    "# Make array that describes Gaussian\n",
    "def make_gaussians(num, mu_min=-1.0, mu_max=1.0, sig_min=0.25, sig_max=4.0): \n",
    "\n",
    "    means = np.random.uniform(mu_min, mu_max, num)\n",
    "    sigmas = np.random.uniform(sig_min, sig_max, num)\n",
    "\n",
    "    x_vals = np.linspace(-10.0, 10.0, 32)\n",
    "    models = np.zeros((num, 32))\n",
    "\n",
    "    for i in range(num):\n",
    "        models[i] = gaussian(x_vals, means[i], sigmas[i])\n",
    "    \n",
    "    # Also want to save and return the true means, sigmas used for the Gaussians\n",
    "    targets = np.vstack((means, sigmas)).T\n",
    "\n",
    "    return models, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Lorentzian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a Lorentzian distribution given its location parameter and scale parameter\n",
    "def lorentzian(x_vals, loc, scale):\n",
    "    return ( 1/(np.pi*scale) ) * ( scale**2 / ((x_vals - loc)**2 + (scale)**2) )\n",
    "\n",
    "def make_lorentzians(num, loc_min=-1.0, loc_max=1.0, scale_min=0.25, scale_max=4.0):\n",
    "    \n",
    "    locs = np.random.uniform(loc_min, loc_max, num)\n",
    "    scales = np.random.uniform(scale_min, scale_max, num)\n",
    "    \n",
    "    x_vals = np.linspace(-10.0, 10.0, 32)\n",
    "    models = np.zeros((num, 32))\n",
    "    \n",
    "    for i in range(num):\n",
    "        models[i] = lorentzian(x_vals, locs[i], scales[i])\n",
    "        \n",
    "    targets = np.vstack((locs, scales)).T\n",
    "    \n",
    "    return models, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot to check Lorentzian function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_vals = np.linspace(-10.0, 10.0, 2**8)\n",
    "\n",
    "plt.plot(x_vals, lorentzian(x_vals, 0.0, 1.0), label=r'x$_{0}=$0, $\\gamma=$1')\n",
    "plt.plot(x_vals, gaussian(x_vals, 0.0, 1.0), label=r'$\\mu=$0, $\\sigma=$1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0.0, 0.45)\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "def generate_dataset(n_train=10000, n_test=1000, frac_gauss=None):\n",
    "    # From desired training and test set sizes, determine the fraction that is Gaussian vs Lorentzian\n",
    "    \n",
    "    # Random fraction w/limits to avoid too many of any one kind\n",
    "    if frac_gauss is None:\n",
    "        fg_train = np.random.uniform(0.25, 0.75)\n",
    "        fg_test = np.random.uniform(0.25, 0.75)\n",
    "    \n",
    "    # User-identified fraction\n",
    "    elif frac_gauss < 1.0:\n",
    "        fg_train = fg_test = frac_gauss\n",
    "        \n",
    "    num_gauss_train = int(fg_train * n_train)\n",
    "    num_gauss_test =  int(fg_test * n_test)\n",
    "    \n",
    "    num_lorentz_train = n_train - num_gauss_train\n",
    "    num_lorentz_test = n_test - num_gauss_test\n",
    "    \n",
    "    print (num_gauss_train, num_lorentz_train, num_gauss_test, num_lorentz_test)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    X_train_gauss, _ = make_gaussians(num_gauss_train)\n",
    "    X_test_gauss, _  = make_gaussians(num_gauss_test)\n",
    "\n",
    "    X_train_lorentz, _ = make_lorentzians(num_lorentz_train)\n",
    "    X_test_lorentz, _ = make_lorentzians(num_lorentz_test)\n",
    "    \n",
    "    # Classification\n",
    "    y_train = np.ones(n_train)\n",
    "    y_train[0:num_gauss_train] *= 0.0 # Gaussians are 0s\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    \n",
    "    y_test = np.ones(n_test)\n",
    "    y_test[0:num_gauss_test] *= 0.0 # Gaussians are 0s\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    # Combine Gaussian and Lorentzians\n",
    "    X_train = np.concatenate((X_train_gauss, X_train_lorentz))\n",
    "    X_test = np.concatenate((X_test_gauss, X_test_lorentz))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, compile, fit, and evaluate NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline model\n",
    "def baseline_model():\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    # There are 2 classes -- Gaussian or Lorentzian -- hence the Dense(2)\n",
    "    model.add(Dense(2, kernel_initializer='normal', activation='softmax')) \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.6381 - accuracy: 0.6972 - val_loss: 0.7793 - val_accuracy: 0.4590\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.5726 - accuracy: 0.7012 - val_loss: 0.7669 - val_accuracy: 0.4590\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.5456 - accuracy: 0.7012 - val_loss: 0.7275 - val_accuracy: 0.4590\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.5084 - accuracy: 0.7032 - val_loss: 0.6641 - val_accuracy: 0.5210\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.4650 - accuracy: 0.7645 - val_loss: 0.6201 - val_accuracy: 0.6220\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.4214 - accuracy: 0.8021 - val_loss: 0.5545 - val_accuracy: 0.6890\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.3763 - accuracy: 0.8322 - val_loss: 0.4933 - val_accuracy: 0.7350\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.3296 - accuracy: 0.8616 - val_loss: 0.4313 - val_accuracy: 0.8310\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.2861 - accuracy: 0.9160 - val_loss: 0.3851 - val_accuracy: 0.8680\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.2513 - accuracy: 0.9356 - val_loss: 0.3386 - val_accuracy: 0.9140\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.2232 - accuracy: 0.9492 - val_loss: 0.3071 - val_accuracy: 0.9240\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.1993 - accuracy: 0.9567 - val_loss: 0.2744 - val_accuracy: 0.9400\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.1789 - accuracy: 0.9656 - val_loss: 0.2544 - val_accuracy: 0.9440\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.1617 - accuracy: 0.9686 - val_loss: 0.2260 - val_accuracy: 0.9580\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.1466 - accuracy: 0.9745 - val_loss: 0.2046 - val_accuracy: 0.9600\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.1335 - accuracy: 0.9771 - val_loss: 0.1879 - val_accuracy: 0.9630\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.1222 - accuracy: 0.9780 - val_loss: 0.1728 - val_accuracy: 0.9720\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.1120 - accuracy: 0.9810 - val_loss: 0.1565 - val_accuracy: 0.9730\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.1030 - accuracy: 0.9824 - val_loss: 0.1453 - val_accuracy: 0.9740\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0951 - accuracy: 0.9836 - val_loss: 0.1333 - val_accuracy: 0.9780\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0881 - accuracy: 0.9859 - val_loss: 0.1243 - val_accuracy: 0.9780\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0816 - accuracy: 0.9873 - val_loss: 0.1158 - val_accuracy: 0.9800\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0759 - accuracy: 0.9885 - val_loss: 0.1061 - val_accuracy: 0.9800\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0706 - accuracy: 0.9896 - val_loss: 0.0990 - val_accuracy: 0.9800\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0659 - accuracy: 0.9904 - val_loss: 0.0921 - val_accuracy: 0.9840\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0615 - accuracy: 0.9926 - val_loss: 0.0857 - val_accuracy: 0.9900\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0576 - accuracy: 0.9933 - val_loss: 0.0806 - val_accuracy: 0.9910\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0540 - accuracy: 0.9940 - val_loss: 0.0752 - val_accuracy: 0.9920\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0506 - accuracy: 0.9942 - val_loss: 0.0705 - val_accuracy: 0.9940\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0475 - accuracy: 0.9953 - val_loss: 0.0663 - val_accuracy: 0.9930\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0448 - accuracy: 0.9953 - val_loss: 0.0627 - val_accuracy: 0.9940\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0421 - accuracy: 0.9965 - val_loss: 0.0586 - val_accuracy: 0.9940\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0396 - accuracy: 0.9963 - val_loss: 0.0548 - val_accuracy: 0.9940\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0373 - accuracy: 0.9967 - val_loss: 0.0514 - val_accuracy: 0.9950\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0352 - accuracy: 0.9968 - val_loss: 0.0489 - val_accuracy: 0.9970\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0333 - accuracy: 0.9969 - val_loss: 0.0469 - val_accuracy: 0.9970\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0314 - accuracy: 0.9972 - val_loss: 0.0442 - val_accuracy: 0.9960\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0296 - accuracy: 0.9972 - val_loss: 0.0408 - val_accuracy: 0.9970\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0281 - accuracy: 0.9973 - val_loss: 0.0390 - val_accuracy: 0.9970\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0265 - accuracy: 0.9976 - val_loss: 0.0372 - val_accuracy: 0.9970\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0251 - accuracy: 0.9975 - val_loss: 0.0352 - val_accuracy: 0.9980\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0238 - accuracy: 0.9978 - val_loss: 0.0336 - val_accuracy: 0.9980\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0225 - accuracy: 0.9980 - val_loss: 0.0314 - val_accuracy: 0.9980\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0213 - accuracy: 0.9980 - val_loss: 0.0303 - val_accuracy: 0.9980\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0203 - accuracy: 0.9983 - val_loss: 0.0282 - val_accuracy: 0.9980\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0192 - accuracy: 0.9984 - val_loss: 0.0265 - val_accuracy: 0.9980\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0183 - accuracy: 0.9986 - val_loss: 0.0253 - val_accuracy: 0.9980\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0173 - accuracy: 0.9984 - val_loss: 0.0243 - val_accuracy: 0.9980\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0165 - accuracy: 0.9987 - val_loss: 0.0227 - val_accuracy: 0.9980\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0157 - accuracy: 0.9989 - val_loss: 0.0220 - val_accuracy: 0.9980\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0149 - accuracy: 0.9989 - val_loss: 0.0212 - val_accuracy: 0.9980\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0142 - accuracy: 0.9990 - val_loss: 0.0200 - val_accuracy: 0.9980\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0135 - accuracy: 0.9993 - val_loss: 0.0189 - val_accuracy: 0.9980\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0129 - accuracy: 0.9993 - val_loss: 0.0180 - val_accuracy: 0.9980\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0122 - accuracy: 0.9993 - val_loss: 0.0168 - val_accuracy: 0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0117 - accuracy: 0.9994 - val_loss: 0.0164 - val_accuracy: 0.9980\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0111 - accuracy: 0.9994 - val_loss: 0.0154 - val_accuracy: 0.9990\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0106 - accuracy: 0.9994 - val_loss: 0.0150 - val_accuracy: 0.9980\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0101 - accuracy: 0.9994 - val_loss: 0.0144 - val_accuracy: 0.9980\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0097 - accuracy: 0.9994 - val_loss: 0.0135 - val_accuracy: 0.9990\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0092 - accuracy: 0.9995 - val_loss: 0.0131 - val_accuracy: 0.9990\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0088 - accuracy: 0.9995 - val_loss: 0.0126 - val_accuracy: 0.9990\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0084 - accuracy: 0.9995 - val_loss: 0.0123 - val_accuracy: 0.9990\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0080 - accuracy: 0.9995 - val_loss: 0.0116 - val_accuracy: 0.9990\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0077 - accuracy: 0.9995 - val_loss: 0.0112 - val_accuracy: 0.9990\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0074 - accuracy: 0.9996 - val_loss: 0.0106 - val_accuracy: 0.9990\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0070 - accuracy: 0.9996 - val_loss: 0.0104 - val_accuracy: 0.9990\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0067 - accuracy: 0.9996 - val_loss: 0.0099 - val_accuracy: 0.9990\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0064 - accuracy: 0.9997 - val_loss: 0.0096 - val_accuracy: 0.9990\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0062 - accuracy: 0.9996 - val_loss: 0.0092 - val_accuracy: 0.9990\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0059 - accuracy: 0.9997 - val_loss: 0.0090 - val_accuracy: 0.9990\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0057 - accuracy: 0.9997 - val_loss: 0.0083 - val_accuracy: 0.9990\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0054 - accuracy: 0.9997 - val_loss: 0.0081 - val_accuracy: 0.9990\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 15us/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 0.0077 - val_accuracy: 0.9990\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0050 - accuracy: 0.9997 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 0.0073 - val_accuracy: 0.9990\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.0046 - accuracy: 0.9997 - val_loss: 0.0069 - val_accuracy: 0.9990\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0044 - accuracy: 0.9997 - val_loss: 0.0065 - val_accuracy: 0.9990\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.0065 - val_accuracy: 0.9990\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0040 - accuracy: 0.9997 - val_loss: 0.0060 - val_accuracy: 0.9990\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.0039 - accuracy: 0.9998 - val_loss: 0.0061 - val_accuracy: 0.9990\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0035 - accuracy: 0.9999 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0034 - accuracy: 0.9998 - val_loss: 0.0051 - val_accuracy: 0.9990\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.0051 - val_accuracy: 0.9990\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 0.9990\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 9us/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 0.9990\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9990\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9990\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 0.9990\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 0.9990\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9990\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 14us/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9990\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 13us/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9990\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 12us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9990\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 10us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9990\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 11us/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 0.9990\n",
      "Baseline Error: 0.10%\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# Create dataset and fit the model\n",
    "X_train, y_train, X_test, y_test = generate_dataset()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128, verbose=1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 8 680 320\n",
      "1000/1000 [==============================] - 0s 24us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0030457507157698274, 0.9990000128746033]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, new_X, new_y = generate_dataset(n_train=10, n_test=1000)\n",
    "model.evaluate(new_X, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify and Regress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "def generate_dataset(n_train=10000, n_test=1000, frac_gauss=None):\n",
    "    # From desired training and test set sizes, determine the fraction that is Gaussian vs Lorentzian\n",
    "    \n",
    "    # Random fraction w/limits to avoid too many of any one kind\n",
    "    if frac_gauss is None:\n",
    "        fg_train = np.random.uniform(0.25, 0.75)\n",
    "        fg_test = np.random.uniform(0.25, 0.75)\n",
    "    \n",
    "    # User-identified fraction\n",
    "    elif frac_gauss < 1.0:\n",
    "        fg_train = fg_test = frac_gauss\n",
    "        \n",
    "    num_gauss_train = int(fg_train * n_train)\n",
    "    num_gauss_test =  int(fg_test * n_test)\n",
    "    \n",
    "    num_lorentz_train = n_train - num_gauss_train\n",
    "    num_lorentz_test = n_test - num_gauss_test\n",
    "    \n",
    "    print (num_gauss_train, num_lorentz_train, num_gauss_test, num_lorentz_test)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    X_train_gauss, y_train_gauss = make_gaussians(num_gauss_train)\n",
    "    X_test_gauss, y_test_gauss = make_gaussians(num_gauss_test)\n",
    "\n",
    "    X_train_lorentz, y_train_lorentz = make_lorentzians(num_lorentz_train)\n",
    "    X_test_lorentz, y_test_lorentz = make_lorentzians(num_lorentz_test)\n",
    "\n",
    "    # Combine Gaussian and Lorentzians\n",
    "    X_train = np.concatenate((X_train_gauss, X_train_lorentz))\n",
    "    y_train = np.concatenate((y_train_gauss, y_train_lorentz))\n",
    "    X_test = np.concatenate((X_test_gauss, X_test_lorentz))\n",
    "    y_test = np.concatenate((y_test_gauss, y_test_lorentz))\n",
    "    \n",
    "    # Want to classify the arrays too...\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
